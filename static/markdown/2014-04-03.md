<!---title:文本处理神器之awk-->
<!---keywords:文本处理,awk,linux命令,awk进阶,awk入门,awk实例-->

称awk为文本处理的神器一点都不为过，感谢Aho、Weinberg、Kernighan，正是这三个人创造了awk---一个优秀的样式扫描与处理工具。

awk 提供了极其强大的功能：可以进行样式装入、流控制、数学运算符、进程控制语句甚至于内置的变量和函数，它具备了一个完整的语言所应具有的几乎所有精美特性。实际上 awk 的确拥有自己的语言：awk 程序设计语言， 三位创建者已将它正式定义为“样式扫描和处理语言”。个人感觉跟PHP语法非常像，变量无类型之分(Typeless)，反而跟shell语法风格不像。

awk非常的快，尤其是大文本的处理，它的优势就越发体现它的价值，现在看下做个1-100000的累加计算，看下它的运算时间：

       
    $ time awk 'BEGIN{ t=0;for(i=0;i<=100000;i++){t+=i;}print t;}'
    5.00005e+09

    real    0m0.018s
    user    0m0.016s
    sys     0m0.000s
    $ time for((i=0, t=0;i<100000;i++));do t=$(($t+$i));done;

    real    0m0.903s
    user    0m0.892s
    sys     0m0.012s

awk之所以那么快，是因为awk采用了yacc的yyparse对awk脚本进行解析并形成node结点树，再由采用了c语言编写并且编译完成的awk主体程序树进行递归遍历。这种执行方式，使得awk既有解释程序的便捷，又有编译程序的效率。如果对shell的简单语法不了解，会对上面和下面的介绍造成障碍，我觉得，很有必要进行下科普，当然也是对我知识库的一个整理。

awk一般格式类似这样  
``awk 'BEGIN {}``  
``pattern1 { actions1 }``  
``pattern2 { actions2 }``  
``...``  
``patternN { actionsN }``  
``END {}' file1 file2 ... fileN``  

`BEGIN`、`END` 就是做读取文件之前和之后做的准备工作和扫尾工作，中间 `pattern { actions }`的就是对行进行遍历，匹配相应的`pattern`则做对应的`actions`。awk是一以行级别扫描，列为字段处理方式进行对文本处理，正是由于awk支持记录和字段的处理，使得awk比grep和sed更为强大。awk的工作流程大概是这样进行的：  
    1、对BEGIN块进行处理  
    2、从file1、file2...依次逐行读入数据（注意的是先读取完file1后再对file2进行读取）  
    3、更新内建变量NR、FNR、$0..$n  
    4、依次进行pattern匹配，匹配返回true，或者大于0的数字、字符串均进行对应的actions块操作 
    5、文件读取完毕之前，依次做2-5操作  

pattern 是什么？跟sed匹配模式是几乎一样的，但比sed强大，支持多种类型的pattern。刚才说了sed支持的，awk也支持。包含`~`， 不包含`!~`，如打出第一个字段含有`9`的行：`awk ' $1 ~ 9 { print ; }' n`。此外，awk支持sed的模式匹配，sed最常用的模式匹配应该就是 ``sed -n 5,10p example`` 或 ``sed -n /regstr/p example`` 写法了，都说了awk是神器，当然要展示下了：

    $ cat n 
    1
    2
    9
    14
    25
    36
    67
    89
    90
    100
    $ awk '/36/,/100/' n   #输出36到100之间的内容
    36
    67
    89
    90
    100

当然awk也支持关系表达式（relational expression），也就是我们常用的 `>`, `<`, `>=`, `<=`, `==`, `!=`，过于简单，但也举个例子，取大于50的数字：`awk '{ if ($1 > 50) print; }' n`和它的简化版本：``awk `$1 > 50 {print;}' n`

一直认为好的例子是最好的老师，希望看一个例子就几乎把基础的awk语法学会。现在我们做一个简单的练习：有一个文本，里面每行有一个数值，要求找到该数值为行数平方的所有行，并计算个数。
     
    $  awk 'BEGIN { len = 0; }
       { 
            if (sqrt($0) == FNR)
            {
             print; len++; 
            }
       }
       END { printf("行内容恰好为行数的平方共有：%d条", len); }' n 
    1
    9
    25
    36
    100
    行内容恰好为行数的平方共有：5条

同样我们也可以这样的来优雅一下：  
``awk 'BEGIN{ len = 0; } sqrt($0) == FNR { print; len++; } END{ printf("行内容恰好为行数的平方共有：%d条", len); }``

这个例子中的pattern就是`sqrt($0) == FNR`，对应的`actions`就是`print; len++;`，里面还涉及的一个awk内置函数（sqrt()）和内置变量（FNR），变量（len）的使用。以此列举内置变量：

    NR：已经读出的记录数
    FNR：当前文件的记录数
    ARGC：命令行参数的个数
    ARGV：命令行参数数组
    ARGIND：当前被处理文件的ARGV标志符
    FS：输入字段分隔符（缺省为:space:），相当于-F选项
    OFS：输出字段分隔符（缺省为:space:）
    NF：当前记录中的字段个数

我觉得这个例子应该很有带入感，至少比一般的入门例子难度大点，实际应用中，我们不会一下子使用那么多特性。看下我们日常的应用就会发现挺简单的，如一般面试的时候都会涉及到日志的统计，下面给两个例子    
取出访问ip前十：``awk  '{print $1}' access.log | sort | uniq -c | sort -k1nr | head 10 ``  
找出不正常状态码：``awk  '$9 !~ /^2/{print $9}' access.log | sort| uniq -c|sort -k1nr``  

接着我们做些进阶的学习，在一些应用中，最为挑战的应该就是大数据了，一些文本有数万数十万行，整个文本体积有好几百M。考验我们的就是对这些命令的了解程度，sed和awk应该采用哪个，组合哪个在前哪个在后，速度和机器压力如何，先看个实例。

两个文件iis（只有一列id）, lls（有5列，其中第4列为id），列出lls第四个字段id和iis里面的id行。实际情境有点复杂，iis文本3780行45K，lls文本2029771行251M。以下做了两种一般解决思路组合处理，猜下哪种时间更少？
     
    $ cat iis
    112233
    223344
    ...
    $ cat lls
    1 2323 4545 112233 http://xx.jpg
    5 2303 4546 223344 http://xx.jpg
    ...

    $ awk '{system("sed -n /"$1"/p lls");}' iis
    $ awk '{if(system("grep -q "$4" iis") == 0) print }' lls

两条都执行一分钟的结果是这样的：  
第一条找出了6条左右  
第二条找出了30条左右  

这结果告诉我们很多东西：即使屠龙刀在我们手里，也能使出菜刀的感觉。遇到大文本的时候，平时我们使用的命令行就得到了最真实的考验，按上面最快的速度也要两天。当然这两个都不是解决办法，至少不是合适的，意味着还得找寻和挖掘awk的进阶用法。上面两个方法慢在于全文查找，无论是sed还是grep，如果把45K的id存起来，那么就会由M*N的数量级降到M，剩下的就是实现方式了。

    $ time awk 'NR==FNR{ids[$1]=$1} NR>FNR && $4 in ids{print}' iis lls > /dev/null 

    real    0m1.389s
    user    0m1.980s
    sys    0m0.180s

结果见证了你对awk的认识，可以在读入第一个文件iis，即判断条件为`NR==FNR`时，把iis中id存起来，在读入第二个文件lls时做匹配`NR>FNR && $4 in ids`时，做`actions`处理。两个文件时`NR>FNR`可以实现判断读入文件先后，对于多个文件作为输入时，此做法失效，可以采用上面内建变量`ARGIND` 来表示当前输入到哪个文件了。也就是说上面语句可改写为：` awk 'ARGIND == 1 {ids[$1]=$1} ARGIND == 2 && $4 in ids{print}' iis lls`。

继续认识下其它的用法吧，现在有一个超大的mysql语句文本，有200多万行，如果要一下子导入的话，机器肯定是要做上好些天，DBA审核也不会通过，需要在每500行处添加个sleep。这样的一个任务应该很简单，方法也很多，我一开始使用sed来做，测试了下速度很慢，看下语句：  
``for i in {1..4000}; do sed -i $((500*$i))asleep iis; done``

再换个思路，把大文本以500行拆分成小文本，然后在每个小文本后面追加sleep，然后再把小文本合并成。  
``split -n500 bigTxt zz;``  
``for i in `ls zz*`;  ``  
``    do    ``  
``       echo sleep > $i;   ``  
``done``  
``cat zz* > bigTxt``

当然，如果用awk的话，事情就变得简单了，耗时3秒以内，也是一条语句：  
``awk '{print > "o"; if (NR % 500 == 0) { print "sleep(1)" > "o" } close("o"); }' bigTxt``

稍微解释下这语句，awk 的 `>` 和标准的shell重定向不一样，虽然在action块里，输出并会覆盖之前的记录，更多的像是 `>>`，意味着上面语句和该语句的实现是一样的：``awk '{print >> "o"; if (NR % 500 == 0) { print "sleep(1)" >> "o" } close("o"); }' bigTxt``。在这样的一个awk特性帮助下，我们做到了优雅的重定向，不需要``awk '{print; if (NR % 500 == 0) { print "sleep(1)" } }' bigTxt  > o``。当然，我们要养成文件操作的标准流程--记得关闭文件。

它的特性也会给我们一般的思维造成麻烦，比如我们想做一个抽取，把上面多列文件lls中的id取出来，输出id（重复则只输出一个），就需要做一个较为特殊的过程来实现。在看这个语句之前我们能想到不下五种语句来实现，这个语句只是用来开阔我们的思维，awk无所不能：``awk '{ a[$4]=$4 } END { for (i in a) print a[i] > "o"} lls'``。

上面的例子已经涉及到了一些不太常见但很给力的awk用法，像多文件操作，重定向。当然，我的博客不会这么忽略我的读者的，会继续完善一些我们更渴望的awk用法。awk的最强大还在于它内建的管道（pipe）功能，意味着我们可以把处理中的数据交由系统的一些命令工具做进一步的加工，这种复用设计思想使得想要完成一项工作，可以在awk的基础上对数据进行任意限定的修剪。

管道多用于BEGIN，且与getline配合，如打印当前时间：`awk 'BEGIN { "date" | getline time; split(time,  arr); printf("%s", arr[4]); }'`。当然在`actions`中也是可以的，如对随机数字进行排序：`awk '{print $1|"sort -k1n"}' nn`，同样有很多的变种，把调用shell的`sort`换成内建函数asort：``awk '{ a[$1] = $1; } END { n = asort(a, b); for(i=1; i<=n; i++) print b[i] }' nn``，得注意到的是数组起始为 1，`asort`两个参数分别为源数组`a`和输出数组`b`，返回值`n`为数组个数。

既然也说到了awk的一个特别内建函数成员`asort`，就得介绍下`asort`的一些相关历史了。`asort`是自gawk 3.1.3起才有的，gawk为何物？且看我下一篇关于gawk的介绍：[翻译和详解gawk 4.0.0特性](http://www.huamanshu.com/blog/2014-04-07.html)，以及[awk脚本进阶攻略](http://www.huamanshu.com/blog/2014-04-09.html)